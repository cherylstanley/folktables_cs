{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code on Anaconda Env.\n",
    "# Construct data source by downloading 2018 data\n",
    "import folktables\n",
    "from folktables import ACSDataSource, ACSIncome\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)  # have to use 'download=True' if data not already avaiable locally\n",
    "\n",
    "features, label, group = ACSIncome.df_to_numpy(acs_data)  # split data into corresponding features, labels, and group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixRaceRates(data, target_bhn_ratio):\n",
    "    # Input: complete training dataset as numpy array\n",
    "    # Output: filtered dataset where ratio of bhn indiviudals satisfies the desired target (+/- 1%)\n",
    "    # RAC1P column # for ASCIncome task -> 9\n",
    "\n",
    "    # count occurances of BHN in the RAC1P column\n",
    "    rac1p_values = data[:, 9]\n",
    "    bhn_count = np.count_nonzero(rac1p_values == 2) + np.count_nonzero(rac1p_values == 3)\n",
    "    \n",
    "    # calc current ratio\n",
    "    current_ratio = bhn_count / len(rac1p_values)\n",
    "\n",
    "    # if the current ratio is within 5% of desired ratio, return the dataset\n",
    "    if abs(current_ratio - target_bhn_ratio) < 0.01:\n",
    "        return data\n",
    "\n",
    "    # if current ratio > target ratio: remove bhn rows\n",
    "    if current_ratio > target_bhn_ratio:\n",
    "        # find indicies of bhn\n",
    "        target_indicies = np.where((rac1p_values == 2) | (rac1p_values == 3))[0]\n",
    "        np.random.shuffle(target_indicies)\n",
    "        # calculate num of bhn rows to remove, then remove specified # of bhn rows\n",
    "        remove_count = bhn_count - int(target_bhn_ratio * len(rac1p_values))\n",
    "        data = np.delete(data, target_indicies[:remove_count], axis=0)\n",
    "    else: \n",
    "    # current ratio < target ratio: remove non-bhn rows\n",
    "        # find indicies of non-bhn\n",
    "        target_indicies = np.where((rac1p_values != 2) & (rac1p_values != 3))[0]\n",
    "        np.random.shuffle(target_indicies)\n",
    "        # calculate num of non-bhn rows to remove, then remove specified # of non-bhn rows\n",
    "        non_bhn_count = len(rac1p_values) - (np.count_nonzero(rac1p_values == 2) + np.count_nonzero(rac1p_values == 3))\n",
    "        remove_count = non_bhn_count - int((1-target_bhn_ratio) * len(rac1p_values))\n",
    "        data = np.delete(data, target_indicies[:remove_count], axis=0)\n",
    "        \n",
    "    # recalc current ratio\n",
    "    '''\n",
    "    rac1p_values = data[:, 9]\n",
    "    bhn_count = np.count_nonzero(rac1p_values == 2) + np.count_nonzero(rac1p_values == 3)\n",
    "    current_ratio = bhn_count / len(rac1p_values)\n",
    "    print(\"updated ratio: \", current_ratio)\n",
    "    '''\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(pred, label):\n",
    "    tn, fp, fn, tp = confusion_matrix(label, pred).ravel()\n",
    "    acc = (tp + tn)/ (tp + tn + fp + fn)\n",
    "    tpr = tp / (tp + fn)\n",
    "    fnr = fn / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    tnr = tn / (fp + tn)\n",
    "    return [acc, tpr, fnr, fpr, tnr]\n",
    "\n",
    "def evaluateRatio(features, label, group, bhn_ratio):\n",
    "    # randomly split the data into training and testing\n",
    "    # train-test split: 80/20\n",
    "    X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "        features, label, group, test_size=0.2)\n",
    "\n",
    "    # reshape y_train into column vector, then concatenate with X_train to reformat training data\n",
    "    train_data = np.concatenate((X_train, y_train.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    # filter training data to satisfy desired BHN ratio\n",
    "    modified_train_data = mixRaceRates(train_data, bhn_ratio)\n",
    "    \n",
    "    new_X_train = modified_train_data[:, :-1] # get all cols except for last one\n",
    "    new_y_train = modified_train_data[:, -1]  # get only the last col\n",
    "\n",
    "    # create the pipeline: normalize data, then use logistic regression as classifier\n",
    "    model = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=16, min_samples_leaf=3))\n",
    "    # train the model with the training data\n",
    "    model.fit(new_X_train, new_y_train)\n",
    "\n",
    "    # make predictions on test data set\n",
    "    yhat = model.predict(X_test)\n",
    "\n",
    "    yhat_wa = yhat[(group_test == 1) | (group_test == 6)]  # all rows from prediction where group=1 (white) or group=6 (asian)\n",
    "    yhat_bhn = yhat[(group_test == 2) | (group_test == 3)]  # all rows from prediction where group=2 (black) or group=3 (american indian)\n",
    "\n",
    "    y_test_wa = y_test[(group_test == 1) | (group_test == 6)]  # all rows from test set where group=1 (white) or group=6 (asian)\n",
    "    y_test_bhn = y_test[(group_test == 2) | (group_test == 3)]  # all rows from test set where group=2 (black) or group=3 (american indian)\n",
    "    \n",
    "    # get the acc, tpr, fnr, fpr, tnr data for WA and BHN groups\n",
    "    wa_data = confusion(yhat_wa, y_test_wa)\n",
    "    bhn_data = confusion(yhat_bhn, y_test_bhn)\n",
    "    \n",
    "    return wa_data, bhn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "% BHN in training set:  5.0\n",
      "Accuracy (WA):  0.8205747877204441\n",
      "TPR (WA):  0.798647579310841\n",
      "FNR (WA):  0.20135242068915904  (Pessimistic Underestimation)\n",
      "FPR (WA):  0.16119385130689634  (Benefit of Doubt)\n",
      "TNR (WA):  0.8388061486931037\n",
      "\n",
      "Accuracy (BHN):  0.7893438579181056\n",
      "TPR (BHN):  0.6873198847262247\n",
      "FNR (BHN):  0.3126801152737752  (Pessimistic Underestimation)\n",
      "FPR (BHN):  0.15753938484621155  (Benefit of Doubt)\n",
      "TNR (BHN):  0.8424606151537885\n",
      "\n",
      "% BHN in training set:  4.0\n",
      "Accuracy (WA):  0.815869918699187\n",
      "TPR (WA):  0.7978004600345026\n",
      "FNR (WA):  0.20219953996549742  (Pessimistic Underestimation)\n",
      "FPR (WA):  0.16920061765055233  (Benefit of Doubt)\n",
      "TNR (WA):  0.8307993823494477\n",
      "\n",
      "Accuracy (BHN):  0.7967091295116773\n",
      "TPR (BHN):  0.7191358024691358\n",
      "FNR (BHN):  0.2808641975308642  (Pessimistic Underestimation)\n",
      "FPR (BHN):  0.16262135922330098  (Benefit of Doubt)\n",
      "TNR (BHN):  0.837378640776699\n",
      "\n",
      "% BHN in training set:  3.0\n",
      "Accuracy (WA):  0.8202782913301547\n",
      "TPR (WA):  0.8078338022073145\n",
      "FNR (WA):  0.19216619779268557  (Pessimistic Underestimation)\n",
      "FPR (WA):  0.16955445544554457  (Benefit of Doubt)\n",
      "TNR (WA):  0.8304455445544554\n",
      "\n",
      "Accuracy (BHN):  0.7843734145104008\n",
      "TPR (BHN):  0.6817496229260935\n",
      "FNR (BHN):  0.31825037707390647  (Pessimistic Underestimation)\n",
      "FPR (BHN):  0.1636085626911315  (Benefit of Doubt)\n",
      "TNR (BHN):  0.8363914373088684\n",
      "\n",
      "% BHN in training set:  2.0\n",
      "Accuracy (WA):  0.8167643229166667\n",
      "TPR (WA):  0.8033093525179856\n",
      "FNR (WA):  0.1966906474820144  (Pessimistic Underestimation)\n",
      "FPR (WA):  0.1721165279429251  (Benefit of Doubt)\n",
      "TNR (WA):  0.8278834720570749\n",
      "\n",
      "Accuracy (BHN):  0.7985537190082644\n",
      "TPR (BHN):  0.7008928571428571\n",
      "FNR (BHN):  0.29910714285714285  (Pessimistic Underestimation)\n",
      "FPR (BHN):  0.1495253164556962  (Benefit of Doubt)\n",
      "TNR (BHN):  0.8504746835443038\n",
      "\n",
      "% BHN in training set:  1.0\n",
      "Accuracy (WA):  0.8147078859441399\n",
      "TPR (WA):  0.8022247905229702\n",
      "FNR (WA):  0.19777520947702976  (Pessimistic Underestimation)\n",
      "FPR (WA):  0.17511629276335158  (Benefit of Doubt)\n",
      "TNR (WA):  0.8248837072366484\n",
      "\n",
      "Accuracy (BHN):  0.7899686520376176\n",
      "TPR (BHN):  0.6947852760736196\n",
      "FNR (BHN):  0.3052147239263804  (Pessimistic Underestimation)\n",
      "FPR (BHN):  0.16085578446909668  (Benefit of Doubt)\n",
      "TNR (BHN):  0.8391442155309033\n",
      "\n",
      "% BHN in training set:  0\n",
      "Accuracy (WA):  0.818486574450773\n",
      "TPR (WA):  0.8013797068123024\n",
      "FNR (WA):  0.1986202931876976  (Pessimistic Underestimation)\n",
      "FPR (WA):  0.16735082396335296  (Benefit of Doubt)\n",
      "TNR (WA):  0.832649176036647\n",
      "\n",
      "Accuracy (BHN):  0.7952101661779081\n",
      "TPR (BHN):  0.725925925925926\n",
      "FNR (BHN):  0.2740740740740741  (Pessimistic Underestimation)\n",
      "FPR (BHN):  0.17067833698030635  (Benefit of Doubt)\n",
      "TNR (BHN):  0.8293216630196937\n"
     ]
    }
   ],
   "source": [
    "target_bhn_ratios = [0.05, 0.04, 0.03, 0.02, 0.01, 0]\n",
    "# map: % bhn in training set -> [ACC, TPR, FNR, FPR, TNR]\n",
    "wa_map = {}\n",
    "bhn_map = {}\n",
    "\n",
    "for target_ratio in target_bhn_ratios:\n",
    "    wa_data, bhn_data = evaluateRatio(features, label, group, target_ratio)\n",
    "    wa_map[target_ratio] = wa_data\n",
    "    bhn_map[target_ratio] = bhn_data\n",
    "\n",
    "# print results\n",
    "for i in range(len(target_bhn_ratios)):\n",
    "    target_ratio = target_bhn_ratios[i]\n",
    "    print(\"\\n% BHN in training set: \", target_ratio * 100)\n",
    "    print(\"Accuracy (WA): \", wa_map[target_ratio][0])\n",
    "    print(\"TPR (WA): \", wa_map[target_ratio][1])\n",
    "    print(\"FNR (WA): \", wa_map[target_ratio][2], \" (Pessimistic Underestimation)\")\n",
    "    print(\"FPR (WA): \", wa_map[target_ratio][3], \" (Benefit of Doubt)\")\n",
    "    print(\"TNR (WA): \", wa_map[target_ratio][4])\n",
    "\n",
    "    print(\"\\nAccuracy (BHN): \", bhn_map[target_ratio][0])\n",
    "    print(\"TPR (BHN): \", bhn_map[target_ratio][1])\n",
    "    print(\"FNR (BHN): \", bhn_map[target_ratio][2], \" (Pessimistic Underestimation)\")\n",
    "    print(\"FPR (BHN): \", bhn_map[target_ratio][3], \" (Benefit of Doubt)\")\n",
    "    print(\"TNR (BHN): \", bhn_map[target_ratio][4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "efcf1752db8516e6a1a861a9a29cd8f0494d7cf6ff0d254dcea6989d66e5bc7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
